# Search-Engine-Optimizer
This project is aimed at creating a simple WebCrawler in Python. From the raw HTML data extracted after crawling through the webpage, the relevant texts are extracted and resturned as a list.

# Description
A web crawler is an automated script to crawl (browse) through pages in the World Wide Web. This is one of the ways used by search engines to keep track of the pages that have been visited and have been optimized.
One of the key challenges that is faced by a web crawl a particular web page and identify its relevancy. Big and powerful search engines perform web crawling where they go around the web searching for pages and the ones are visited are marked as ‘visited’. To optimize their search, they have also started classifying the pages based on the content, to easily identify and filter out results based on key search terms.

# Objective
To take any URL, scrape it and give the most relevant details to classify it and enable in search optimization.

# Approach
A generic URL is taken as a user input. The resultant is the most relavant key words which are scraped from the web data. The relavant key words are the cleaned words using NLTK package, and the most frequently occurring words.

# Links
1. [Scripts](https://github.com/arvindv17/Search-Engine-Optimizer/tree/master/Scripts)
2. [Documents](https://github.com/arvindv17/Search-Engine-Optimizer/tree/master/Documents)
